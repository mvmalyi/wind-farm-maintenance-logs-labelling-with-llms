{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175708f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2_analyse_results.ipynb\n",
    "#\n",
    "# This notebook loads the results from the benchmark runs, performs\n",
    "# detailed analysis on performance and accuracy, and generates\n",
    "# publication-ready tables and figures.\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef91363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "\n",
    "# --- Matplotlib and Seaborn Configuration ---\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# --- Warnings Management ---\n",
    "warnings.simplefilter(\"ignore\")   #to disable warnings\n",
    "#warnings.simplefilter(\"default\") #to enable warnings\n",
    "#warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib') #for matplotlib only\n",
    "\n",
    "print(\"INFO: Imports and settings loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis Configuration ===\n",
    "\n",
    "# A list of run_id strings to be combined and analysed.\n",
    "RUN_IDS_TO_ANALYSE = [\"run_20250814_150749\", \"run_20250814_211348\", \"run_20250814_154132\"] # Different runs (e.g. GPT, Gemini, and Ollama) can be imported separately\n",
    "\n",
    "MODELS_TO_EXCLUDE = [\"gemini-2.5-flash-lite\"] # a model excluded from the analysis for the time being (significantly faster than any other but not reliable)\n",
    "\n",
    "# The ground truth to use for accuracy and F1-score calculations.\n",
    "GOLDEN_DATASET = \"gpt-5\"\n",
    "\"\"\"\n",
    "Choose the source for the 'golden standard' labels.\n",
    "Options:\n",
    "- \"Manual\": Use the manually labelled golden dataset file. (Measures true accuracy).\n",
    "- \"Consensus\": Use the majority vote among all models as the ground truth. (Measures agreement with the crowd).\n",
    "- \"<model_name>\": Use a specific model's output (e.g., \"gpt-5\" as the best performing one) as the ground truth. (Measures alignment with that model).\n",
    "- None: Skip the accuracy-based analysis.\n",
    "\"\"\"\n",
    "\n",
    "# --- Add paths to configuration files ---\n",
    "# Path to the config file containing pricing and model info\n",
    "CONFIG_FILE = \"configurations/config.yaml\"\n",
    "# Path to the original dataset (used if manual labels are needed)\n",
    "MANUAL_LABELS_FILE = \"input_dataset.csv\" # columns with names 'Golden_Maint_Type' and 'Golden_Issue_Cat' would be used as the ground truth if present\n",
    "# Path to the initial prompt and labels\n",
    "PROMPTS_FILE = \"prompts.json\"\n",
    "\n",
    "\n",
    "# Generate a unique ID for this specific analysis execution\n",
    "analysis_id = f\"analysis_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "# Define the path to the unique output directory\n",
    "ANALYSIS_OUTPUT_DIR = Path(\"outputs/analysis_results\") / analysis_id\n",
    "# Create the directory\n",
    "ANALYSIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"INFO: Analysis outputs will be saved to: {ANALYSIS_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# --- Helper function to load the config ---\n",
    "def load_config(config_path=\"config.yaml\"):\n",
    "    \"\"\"Loads the YAML configuration file.\"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# --- Load the config file into memory ---\n",
    "try:\n",
    "    config = load_config(CONFIG_FILE)\n",
    "    print(f\"INFO: Configuration loaded from '{CONFIG_FILE}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Configuration file is not found at '{CONFIG_FILE}'. The default config.yaml is absent. Aborting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a24f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading and Merging ===\n",
    "\n",
    "def load_and_merge_runs_safely(run_ids: list, base_data_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a clean master DataFrame using a memory-safe strategy that\n",
    "    de-duplicates all data sources before merging to prevent memory explosion which happened in earlier versions.\n",
    "    \"\"\"\n",
    "    if not run_ids:\n",
    "        raise ValueError(\"ERROR: run_ids list cannot be empty.\")\n",
    "\n",
    "    # 1. Load the base data and immediately de-duplicate it.\n",
    "    try:\n",
    "        master_df = pd.read_csv(base_data_file)\n",
    "        master_df.drop_duplicates(subset=['WONUM'], keep='first', inplace=True)\n",
    "        print(f\"INFO: Loaded and de-duplicated clean base data from '{base_data_file}'.\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The base data file '{base_data_file}' was not found.\")\n",
    "\n",
    "    loaded_models = set()\n",
    "    \n",
    "    # 2. Loop through each run file.\n",
    "    for run_id in run_ids:\n",
    "        run_path = Path(\"outputs\") / \"runs\" / run_id / \"Master_Benchmark_Results.csv\"\n",
    "        if not run_path.exists():\n",
    "            print(f\"WARNING: Results file for run_id '{run_id}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"INFO: Processing run '{run_id}'...\")\n",
    "        run_df = pd.read_csv(run_path, on_bad_lines='skip')\n",
    "        \n",
    "        # 3. De-duplicate this run's data immediately after loading\n",
    "        run_df.drop_duplicates(subset=['WONUM'], keep='first', inplace=True)\n",
    "        \n",
    "        # Identify models present in this run file\n",
    "        models_in_run = sorted(list(set([col.split('_Maint_Type')[0] for col in run_df.columns if '_Maint_Type' in col])))\n",
    "        \n",
    "        for model_name in models_in_run:\n",
    "            if model_name in loaded_models:\n",
    "                raise ValueError(f\"Duplicate model '{model_name}' found. Please provide run_ids with unique models.\")\n",
    "            \n",
    "            # Grab only the columns for this specific model\n",
    "            cols_for_this_model = [col for col in run_df.columns if col.startswith(model_name + '_')]\n",
    "            model_data_to_merge = run_df[['WONUM'] + cols_for_this_model]\n",
    "            \n",
    "            # 4. Merge the lean, de-duplicated model data into the master DataFrame.\n",
    "            master_df = pd.merge(master_df, model_data_to_merge, on=\"WONUM\", how=\"left\")\n",
    "            loaded_models.add(model_name)\n",
    "            \n",
    "    print(\"\\nINFO: All run data successfully loaded and merged.\")\n",
    "    return master_df\n",
    "\n",
    "# --- Load the data using the safe function ---\n",
    "master_df = load_and_merge_runs_safely(RUN_IDS_TO_ANALYSE, MANUAL_LABELS_FILE)\n",
    "print(\"INFO: Raw master DataFrame loaded. Cleanup will be performed in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Data Cleaning and Preparation ===\n",
    "\n",
    "# 1. Drop unnecessary columns to reduce memory footprint\n",
    "cols_to_drop = ['Farm ID', 'Farm Name', 'Farm Number', 'Turbine No', 'Age at Event', 'Event Date'] # Add any other columns that are not needed for analysis\n",
    "master_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "print(\"INFO: Dropped unnecessary columns.\")\n",
    "\n",
    "# 2. Clean up suffixes created by the merge\n",
    "cols_x = [col for col in master_df.columns if col.endswith('_x')]\n",
    "if cols_x:\n",
    "    print(f\"INFO: Found {len(cols_x)} columns with '_x' suffix. Consolidating...\")\n",
    "    cols_to_drop_after_cleanup = []\n",
    "    for col_x in cols_x:\n",
    "        base_name = col_x[:-2]\n",
    "        col_y = base_name + '_y'\n",
    "        \n",
    "        if col_y in master_df.columns:\n",
    "            master_df[base_name] = master_df[col_x].combine_first(master_df[col_y])\n",
    "            cols_to_drop_after_cleanup.extend([col_x, col_y])\n",
    "        else:\n",
    "            master_df.rename(columns={col_x: base_name}, inplace=True)\n",
    "            \n",
    "    master_df.drop(columns=cols_to_drop_after_cleanup, inplace=True, errors='ignore')\n",
    "    print(\"INFO: Suffix cleanup complete.\")\n",
    "\n",
    "# 3. De-duplicate the DataFrame to ensure accurate sums\n",
    "initial_rows = len(master_df)\n",
    "master_df.drop_duplicates(subset=['WONUM'], keep='first', inplace=True)\n",
    "final_rows = len(master_df)\n",
    "if initial_rows > final_rows:\n",
    "    print(f\"INFO: Removed {initial_rows - final_rows} duplicate rows based on 'WONUM'.\")\n",
    "\n",
    "# Finalize the list of model names, applying the exclusion list\n",
    "all_models_found = sorted([col.split('_Maint_Type')[0] for col in master_df.columns if '_Maint_Type' in col])\n",
    "model_names = [model for model in all_models_found if model not in MODELS_TO_EXCLUDE]\n",
    "\n",
    "print(f\"INFO: Found {len(all_models_found)} total models in the data.\")\n",
    "if MODELS_TO_EXCLUDE:\n",
    "    print(f\"INFO: Excluding {len(MODELS_TO_EXCLUDE)} models: {MODELS_TO_EXCLUDE}\")\n",
    "print(f\"INFO: Final {len(model_names)} models for analysis: {model_names}\")\n",
    "\n",
    "# Display a sample of the cleaned, merged data\n",
    "display(master_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ground Truth Preparation ===\n",
    "\n",
    "run_accuracy_analysis = True\n",
    "ground_truth_source = \"\" # This variable stores a descriptive name for the ground truth source to use in table/figure titles.\n",
    "\n",
    "if GOLDEN_DATASET is None:\n",
    "    run_accuracy_analysis = False\n",
    "    print(\"INFO: `GOLDEN_DATASET` is None. Skipping accuracy-based analysis.\")\n",
    "\n",
    "elif GOLDEN_DATASET == \"Manual\":\n",
    "    print(\"INFO: Using 'Manual' labels as the ground truth.\")\n",
    "    try:\n",
    "        manual_df = pd.read_csv(MANUAL_LABELS_FILE)\n",
    "        ground_truth_df = manual_df[['WONUM', 'Golden_Maint_Type', 'Golden_Issue_Cat']].copy()\n",
    "        ground_truth_source = \"Manual Labels\"\n",
    "        \n",
    "        # De-duplicate the manual labels to ensure a clean merge.\n",
    "        initial_rows = len(ground_truth_df)\n",
    "        ground_truth_df.drop_duplicates(subset=['WONUM'], keep='first', inplace=True)\n",
    "        if len(ground_truth_df) < initial_rows:\n",
    "            print(f\"INFO: Removed {initial_rows - len(ground_truth_df)} duplicate entries from the manual labels file.\")\n",
    "\n",
    "        # The merge is now safe\n",
    "        master_df = pd.merge(master_df, ground_truth_df, on=\"WONUM\", how=\"left\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or merge manual labels. {e}\")\n",
    "        run_accuracy_analysis = False\n",
    "\n",
    "elif GOLDEN_DATASET == \"Consensus\":\n",
    "    print(\"INFO: Using 'Consensus' (majority vote) as the ground truth.\")\n",
    "    maint_type_cols = [f\"{m}_Maint_Type\" for m in model_names]\n",
    "    issue_cat_cols = [f\"{m}_Issue_Cat\" for m in model_names]\n",
    "    \n",
    "    # --- Create columns directly instead of merging ---\n",
    "    master_df['Golden_Maint_Type'] = master_df[maint_type_cols].mode(axis=1)[0]\n",
    "    master_df['Golden_Issue_Cat'] = master_df[issue_cat_cols].mode(axis=1)[0]\n",
    "    ground_truth_source = \"Consensus Label\"\n",
    "\n",
    "else: # A specific model name was given\n",
    "    model_name = GOLDEN_DATASET\n",
    "    print(f\"INFO: Using '{model_name}' output as the ground truth.\")\n",
    "    maint_col = f\"{model_name}_Maint_Type\"\n",
    "    issue_col = f\"{model_name}_Issue_Cat\"\n",
    "    if maint_col not in master_df.columns or issue_col not in master_df.columns:\n",
    "        print(f\"ERROR: Model '{model_name}' not found in the loaded data. Cannot use as ground truth.\")\n",
    "        run_accuracy_analysis = False\n",
    "    else:\n",
    "        # --- Create columns directly instead of merging ---\n",
    "        master_df['Golden_Maint_Type'] = master_df[maint_col]\n",
    "        master_df['Golden_Issue_Cat'] = master_df[issue_col]\n",
    "        ground_truth_source = f\"{model_name} Labels\"\n",
    "\n",
    "if run_accuracy_analysis:\n",
    "    print(f\"INFO: Ground truth from '{ground_truth_source}' prepared successfully.\")\n",
    "    # Drop rows where ground truth is missing, as they cannot be evaluated\n",
    "    master_df.dropna(subset=['Golden_Maint_Type', 'Golden_Issue_Cat'], inplace=True)\n",
    "    print(f\"INFO: After dropping rows with missing ground truth, {len(master_df)} rows remain for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Validation: Check for Invalid (Hallucinated) Labels ===\n",
    "\n",
    "print(\"Data Validation: Checking for Invalid Labels and Errors\\n\")\n",
    "\n",
    "# 1. Get the master list of all valid labels from the prompts file\n",
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_config = json.load(f)\n",
    "\n",
    "valid_maint_types = set(prompts_config[\"maintenance_types\"].keys())\n",
    "valid_issue_cats = set(prompts_config[\"issue_categories\"].keys())\n",
    "technical_error_strings = {\"CLIENT_ERROR\", \"FORMATTING_ERROR\"}\n",
    "\n",
    "any_issue_found = False\n",
    "# 2. Loop through each model and check its output\n",
    "for model in model_names:\n",
    "    model_had_issue = False\n",
    "    \n",
    "    # Get all unique labels produced by this model\n",
    "    produced_maint_types = set(master_df[f\"{model}_Maint_Type\"].unique())\n",
    "    produced_issue_cats = set(master_df[f\"{model}_Issue_Cat\"].unique())\n",
    "\n",
    "    # --- Check for technical errors ---\n",
    "    found_maint_techn_errors = produced_maint_types.intersection(technical_error_strings)\n",
    "    if found_maint_techn_errors:\n",
    "        any_issue_found = model_had_issue = True\n",
    "        print(f\"ðŸ”µ INFO: Model '{model}' contains technical Maintenance Type errors: {', '.join(found_maint_techn_errors)}\")\n",
    "\n",
    "    found_issue_techn_errors = produced_issue_cats.intersection(technical_error_strings)\n",
    "    if found_issue_techn_errors:\n",
    "        any_issue_found = model_had_issue = True\n",
    "        print(f\"ðŸ”µ INFO: Model '{model}' contains technical Issue Category errors: {', '.join(found_issue_techn_errors)}\")\n",
    "\n",
    "    # --- Check for content errors (hallucinations) ---\n",
    "    invalid_maint = produced_maint_types - valid_maint_types - technical_error_strings\n",
    "    invalid_issue = produced_issue_cats - valid_issue_cats - technical_error_strings\n",
    "\n",
    "    if invalid_maint:\n",
    "        any_issue_found = model_had_issue = True\n",
    "        print(f\"ðŸ”´ WARNING: Model '{model}' produced {len(invalid_maint)} invalid Maintenance Types:\")\n",
    "        print(f\"   - {', '.join(str(i) for i in invalid_maint if pd.notna(i))}\")\n",
    "        \n",
    "    if invalid_issue:\n",
    "        any_issue_found = model_had_issue = True\n",
    "        print(f\"ðŸ”´ WARNING: Model '{model}' produced {len(invalid_issue)} invalid Issue Categories:\")\n",
    "        print(f\"   - {', '.join(str(i) for i in invalid_issue if pd.notna(i))}\")\n",
    "        \n",
    "    if model_had_issue:\n",
    "        print(\"\") # Add a newline for readability between models with issues\n",
    "\n",
    "if not any_issue_found:\n",
    "    print(\"ðŸŸ¢ All models adhered to the provided labels and produced no errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Performance Metrics Analysis ===\n",
    "\n",
    "print(\"Table 1: Overall Performance Summary\")\n",
    "\n",
    "# Get the master list of valid labels once before the loop\n",
    "valid_maint_types = set(prompts_config[\"maintenance_types\"].keys())\n",
    "valid_issue_cats = set(prompts_config[\"issue_categories\"].keys())\n",
    "\n",
    "perf_results = []\n",
    "for model in model_names:\n",
    "    # --- Comprehensive Error Rate Calculation ---\n",
    "    maint_col = master_df[f\"{model}_Maint_Type\"]\n",
    "    \n",
    "    # 1. Count technical errors (CLIENT_ERROR, FORMATTING_ERROR)\n",
    "    technical_errors = maint_col.isin([\"CLIENT_ERROR\", \"FORMATTING_ERROR\"])\n",
    "    \n",
    "    # 2. Count content errors (hallucinated labels)\n",
    "    # A row is a content error if it's NOT a technical error AND it's NOT a valid label\n",
    "    content_errors = ~technical_errors & ~maint_col.isin(valid_maint_types)\n",
    "    \n",
    "    # 3. Total error is the sum of both types\n",
    "    total_error_count = technical_errors.sum() + content_errors.sum()\n",
    "    total_logs = len(master_df)\n",
    "    error_rate = (total_error_count / total_logs) * 100 if total_logs > 0 else 0\n",
    "    \n",
    "    # Tokens and Cost\n",
    "    prompt_tokens = master_df[f\"{model}_Prompt_Tokens\"].sum()\n",
    "    comp_tokens = master_df[f\"{model}_Comp_Tokens\"].sum()\n",
    "    \n",
    "    # Find model config to get pricing\n",
    "    config_model_name = model.replace('_', ':')\n",
    "    model_cfg = next((item for item in config[\"models_to_benchmark\"] if item[\"model_name\"] == config_model_name), None)\n",
    "    \n",
    "    if model_cfg is None:\n",
    "        print(f\"WARNING: No configuration found for model '{model}' in {CONFIG_FILE}. Skipping performance calculation.\")\n",
    "        continue\n",
    "\n",
    "    price_input = model_cfg.get(\"pricing\", {}).get(\"individual\", {}).get(\"input\", 0.0)\n",
    "    price_output = model_cfg.get(\"pricing\", {}).get(\"individual\", {}).get(\"output\", 0.0)\n",
    "    cost = ((prompt_tokens / 1_000_000) * price_input) + ((comp_tokens / 1_000_000) * price_output)\n",
    "\n",
    "    # Throughput\n",
    "    perf_log = pd.read_csv(\"outputs/performance_log.csv\")\n",
    "    model_perf_rows = perf_log[perf_log[\"Model\"] == config_model_name]\n",
    "    if not model_perf_rows.empty:\n",
    "        model_perf = model_perf_rows.iloc[-1]\n",
    "        throughput = model_perf[\"Performance (logs/s)\"]\n",
    "    else:\n",
    "        throughput = 0\n",
    "\n",
    "    perf_results.append({\n",
    "        \"Model\": model,\n",
    "        \"Throughput (logs/s)\": throughput,\n",
    "        \"Total Tokens\": prompt_tokens + comp_tokens,\n",
    "        \"Estimated Cost ($)\": cost,\n",
    "        \"Error Rate (%)\": error_rate\n",
    "    })\n",
    "\n",
    "perf_summary_df = pd.DataFrame(perf_results).set_index(\"Model\")\n",
    "display(perf_summary_df.style.format({\n",
    "    \"Throughput (logs/s)\": \"{:.2f}\",\n",
    "    \"Estimated Cost ($)\": \"${:.4f}\",\n",
    "    \"Error Rate (%)\": \"{:.2f}%\"\n",
    "}))\n",
    "\n",
    "# --- Figure 1: Throughput (Processing Speed) ---\n",
    "print(\"\\nFigure 1: Processing Speed Comparison\")\n",
    "plt.figure(figsize=(12, 7))\n",
    "perf_summary_df.sort_values(\"Throughput (logs/s)\", ascending=False)['Throughput (logs/s)'].plot(\n",
    "    kind='bar', color=sns.color_palette(\"viridis\", len(perf_summary_df))\n",
    ")\n",
    "plt.title(\"Model Throughput (Processing Speed)\")\n",
    "plt.ylabel(\"Logs Processed per Second\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_1.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- Figure 2: Estimated Cost ---\n",
    "print(\"\\nFigure 2: Estimated Cost Comparison\")\n",
    "\n",
    "# Filter the dataframe to only include models with a cost greater than zero\n",
    "cost_df = perf_summary_df[perf_summary_df[\"Estimated Cost ($)\"] > 0]\n",
    "\n",
    "if not cost_df.empty:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    cost_df.sort_values(\"Estimated Cost ($)\", ascending=False)['Estimated Cost ($)'].plot(\n",
    "        kind='bar', color=sns.color_palette(\"plasma\", len(cost_df))\n",
    "    )\n",
    "    plt.title(\"Estimated Cost to Process Dataset (API Models Only)\")\n",
    "    plt.ylabel(\"Cost (USD)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_2.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"INFO: No models with a non-zero cost were found to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classification Accuracy Analysis ===\n",
    "\n",
    "if not run_accuracy_analysis:\n",
    "    print(\"INFO: Skipping Classification Accuracy Analysis as `GOLDEN_DATASET` was not set to a valid source.\")\n",
    "else:\n",
    "    print(f\"Table 2: Classification Metrics (vs. {ground_truth_source})\")\n",
    "    \n",
    "    # Create a new list of models to evaluate, excluding the ground truth model if it's one of them.\n",
    "    models_to_evaluate = model_names.copy()\n",
    "    if GOLDEN_DATASET in models_to_evaluate:\n",
    "        print(f\"INFO: Excluding '{GOLDEN_DATASET}' from the comparison as it is the selected ground truth.\")\n",
    "        models_to_evaluate.remove(GOLDEN_DATASET)\n",
    "    \n",
    "    accuracy_results = []\n",
    "\n",
    "    # The loop now iterates over the filtered list\n",
    "    for model in models_to_evaluate:\n",
    "        # Filter out error rows for this model before calculating accuracy\n",
    "        eval_df = master_df[~master_df[f\"{model}_Maint_Type\"].isin([\"CLIENT_ERROR\", \"FORMATTING_ERROR\"])].copy()\n",
    "        \n",
    "        # Drop rows with NaNs in either the prediction or true label for fair comparison\n",
    "        eval_df.dropna(subset=['Golden_Maint_Type', f\"{model}_Maint_Type\"], inplace=True)\n",
    "        y_true_maint = eval_df['Golden_Maint_Type']\n",
    "        y_pred_maint = eval_df[f\"{model}_Maint_Type\"]\n",
    "        maint_f1 = f1_score(y_true_maint, y_pred_maint, average='weighted', zero_division=0)\n",
    "        maint_acc = accuracy_score(y_true_maint, y_pred_maint)\n",
    "\n",
    "        # Do the same for Issue Category\n",
    "        eval_df.dropna(subset=['Golden_Issue_Cat', f\"{model}_Issue_Cat\"], inplace=True)\n",
    "        y_true_issue = eval_df['Golden_Issue_Cat']\n",
    "        y_pred_issue = eval_df[f\"{model}_Issue_Cat\"]\n",
    "        issue_f1 = f1_score(y_true_issue, y_pred_issue, average='weighted', zero_division=0)\n",
    "        issue_acc = accuracy_score(y_true_issue, y_pred_issue)\n",
    "        \n",
    "        accuracy_results.append({\n",
    "            \"Model\": model,\n",
    "            \"Maint. Type F1\": maint_f1,\n",
    "            \"Maint. Type Acc.\": maint_acc,\n",
    "            \"Issue Cat. F1\": issue_f1,\n",
    "            \"Issue Cat. Acc.\": issue_acc\n",
    "        })\n",
    "        \n",
    "    accuracy_summary_df = pd.DataFrame(accuracy_results).set_index(\"Model\")\n",
    "    if not accuracy_summary_df.empty:\n",
    "        display(accuracy_summary_df.style.format(\"{:.3f}\"))\n",
    "\n",
    "        # --- Figure 3: F1-Score Comparison ---\n",
    "        print(f\"\\nFigure 3: F1-Score Comparison (vs. {ground_truth_source})\")\n",
    "        accuracy_summary_df[['Maint. Type F1', 'Issue Cat. F1']].sort_values('Maint. Type F1', ascending=False).plot(\n",
    "            kind='bar', figsize=(14, 7)\n",
    "        )\n",
    "        plt.title(\"Weighted F1-Scores for Classification Tasks\")\n",
    "        plt.ylabel(\"F1-Score\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(title=\"Task\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_3.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # --- Figure 4: Confusion Matrices (Separate Figures) ---\n",
    "        print(f\"\\nFigure 4: Confusion Matrices (vs. {ground_truth_source})\")\n",
    "        top_3_models = accuracy_summary_df.sort_values('Maint. Type F1', ascending=False).head(3).index\n",
    "\n",
    "        for model in top_3_models:\n",
    "            print(f\"\\n--- Generating confusion matrices for: {model} ---\")\n",
    "            \n",
    "            # --- Plot 1: Maintenance Type ---\n",
    "            plt.figure(figsize=(16, 14)) # Create a dedicated figure for this plot\n",
    "            \n",
    "            temp_df_maint = master_df[['Golden_Maint_Type', f\"{model}_Maint_Type\"]].dropna()\n",
    "            maint_labels = pd.unique(temp_df_maint[['Golden_Maint_Type', f\"{model}_Maint_Type\"]].values.ravel('K'))\n",
    "            cm_maint = confusion_matrix(temp_df_maint['Golden_Maint_Type'], temp_df_maint[f\"{model}_Maint_Type\"], labels=maint_labels)\n",
    "            \n",
    "            sns.heatmap(cm_maint, annot=True, fmt='d', cmap='Blues', \n",
    "                        xticklabels=maint_labels, yticklabels=maint_labels,\n",
    "                        annot_kws={\"size\": 10})\n",
    "                        \n",
    "            plt.title(f\"Maintenance Type Confusion Matrix for: {model}\", fontsize=18)\n",
    "            plt.ylabel(\"True Label\", fontsize=14)\n",
    "            plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_4_1.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            # --- Plot 2: Issue Category ---\n",
    "            plt.figure(figsize=(22, 18)) # Create a second, larger figure for this plot\n",
    "            \n",
    "            temp_df_issue = master_df[['Golden_Issue_Cat', f\"{model}_Issue_Cat\"]].dropna()\n",
    "            issue_labels = pd.unique(temp_df_issue[['Golden_Issue_Cat', f\"{model}_Issue_Cat\"]].values.ravel('K'))\n",
    "            cm_issue = confusion_matrix(temp_df_issue['Golden_Issue_Cat'], temp_df_issue[f\"{model}_Issue_Cat\"], labels=issue_labels)\n",
    "\n",
    "            sns.heatmap(cm_issue, annot=True, fmt='d', cmap='Reds',\n",
    "                    xticklabels=issue_labels, yticklabels=issue_labels,\n",
    "                    annot_kws={\"size\": 10})\n",
    "\n",
    "            plt.title(f\"Issue Category Confusion Matrix for: {model}\", fontsize=18)\n",
    "            plt.ylabel(\"Assumed Ground Truth Label\", fontsize=14)\n",
    "            plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_4_2.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"INFO: No models available for accuracy comparison after excluding the ground truth model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e88e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Confidence Level Analysis ===\n",
    "\n",
    "print(\"Figure 5: Model Self-Reported Confidence Distribution\")\n",
    "\n",
    "confidence_cols = [f\"{m}_Certainty\" for m in model_names]\n",
    "confidence_df = master_df[confidence_cols].copy()\n",
    "confidence_df.columns = model_names\n",
    "\n",
    "# Calculate percentage distribution\n",
    "confidence_dist = confidence_df.apply(lambda x: x.value_counts(normalize=True)).T\n",
    "confidence_dist = confidence_dist.reindex(columns=['High', 'Medium', 'Low']).fillna(0) \n",
    "\n",
    "confidence_dist.plot(\n",
    "    kind='bar', stacked=True, figsize=(14, 7),\n",
    "    color=['#2ca02c', '#ff7f0e', '#d62728'] # Green, Orange, Red\n",
    ")\n",
    "plt.title(\"Distribution of Self-Reported Confidence Levels\")\n",
    "plt.ylabel(\"Proportion of Predictions\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"Confidence Level\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_5.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- F1-Score vs. Confidence ---\n",
    "if run_accuracy_analysis:\n",
    "    print(f\"\\nTables 3 & 4 and Figures 6 & 7: F1-Score vs. Confidence (Ground Truth: {ground_truth_source})\")\n",
    "\n",
    "    confidence_accuracy_results = []\n",
    "    for model in model_names:\n",
    "        if model == GOLDEN_DATASET:\n",
    "            continue\n",
    "            \n",
    "        for level in ['High', 'Medium', 'Low']:\n",
    "            subset_df = master_df[master_df[f\"{model}_Certainty\"] == level]\n",
    "            if not subset_df.empty:\n",
    "                # Calculate F1 for Maintenance Type\n",
    "                df_maint = subset_df.dropna(subset=['Golden_Maint_Type', f\"{model}_Maint_Type\"])\n",
    "                f1_maint = f1_score(df_maint['Golden_Maint_Type'], df_maint[f\"{model}_Maint_Type\"], average='weighted', zero_division=0)\n",
    "                \n",
    "                # --- Calculate F1 for Issue Category ---\n",
    "                df_issue = subset_df.dropna(subset=['Golden_Issue_Cat', f\"{model}_Issue_Cat\"])\n",
    "                f1_issue = f1_score(df_issue['Golden_Issue_Cat'], df_issue[f\"{model}_Issue_Cat\"], average='weighted', zero_division=0)\n",
    "                \n",
    "                confidence_accuracy_results.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Confidence\": level,\n",
    "                    \"F1 Score (Maint. Type)\": f1_maint,\n",
    "                    \"F1 Score (Issue Cat.)\": f1_issue # Add to results\n",
    "                })\n",
    "    \n",
    "    if confidence_accuracy_results:\n",
    "        conf_acc_df = pd.DataFrame(confidence_accuracy_results)\n",
    "        \n",
    "        # --- Create and display table/plot for Maintenance Type ---\n",
    "        print(\"\\nTable 3: F1-Score (Maint. Type) vs. Confidence\")\n",
    "        conf_acc_pivot_maint = conf_acc_df.pivot(index='Model', columns='Confidence', values='F1 Score (Maint. Type)')\n",
    "        display(conf_acc_pivot_maint.style.format(\"{:.3f}\", na_rep=\"-\").background_gradient(cmap='viridis'))\n",
    "        \n",
    "        conf_acc_pivot_maint.plot(kind='bar', figsize=(14,7))\n",
    "        plt.title(f\"F1-Score (Maint. Type) Stratified by Confidence (vs. {ground_truth_source})\")\n",
    "        plt.ylabel(\"Weighted F1-Score\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0,1)\n",
    "        #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_6.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        # --- Create and display table/plot for Issue Category ---\n",
    "        print(\"\\nTable 4: F1-Score (Issue Cat.) vs. Confidence\")\n",
    "        conf_acc_pivot_issue = conf_acc_df.pivot(index='Model', columns='Confidence', values='F1 Score (Issue Cat.)')\n",
    "        display(conf_acc_pivot_issue.style.format(\"{:.3f}\", na_rep=\"-\").background_gradient(cmap='plasma'))\n",
    "\n",
    "        conf_acc_pivot_issue.plot(kind='bar', figsize=(14,7))\n",
    "        plt.title(f\"F1-Score (Issue Cat.) Stratified by Confidence (vs. {ground_truth_source})\")\n",
    "        plt.ylabel(\"Weighted F1-Score\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0,1)\n",
    "        #plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_7.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"INFO: No data available for confidence vs. accuracy analysis.\")\n",
    "else:\n",
    "    print(\"INFO: Skipping F1-Score vs. Confidence analysis because no ground truth is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a72541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Consensus and Inter-Model Agreement ===\n",
    "\n",
    "print(\"Table 5: Model Agreement with Consensus\")\n",
    "\n",
    "maint_type_cols = [f\"{m}_Maint_Type\" for m in model_names]\n",
    "issue_cat_cols = [f\"{m}_Issue_Cat\" for m in model_names]\n",
    "\n",
    "# Create temporary Series for the consensus labels\n",
    "consensus_maint = master_df[maint_type_cols].mode(axis=1)[0]\n",
    "consensus_issue = master_df[issue_cat_cols].mode(axis=1)[0]\n",
    "\n",
    "agreement_results = []\n",
    "for model in model_names:\n",
    "    # Create a temporary dataframe with the necessary columns\n",
    "    temp_df = pd.DataFrame({\n",
    "        'consensus_maint': consensus_maint,\n",
    "        'model_maint': master_df[f\"{model}_Maint_Type\"],\n",
    "        'consensus_issue': consensus_issue,\n",
    "        'model_issue': master_df[f\"{model}_Issue_Cat\"]\n",
    "    }).dropna() # Drop rows where any value is NaN\n",
    "\n",
    "    # Calculate scores on the cleaned data\n",
    "    agree_maint = accuracy_score(temp_df['consensus_maint'], temp_df['model_maint'])\n",
    "    agree_issue = accuracy_score(temp_df['consensus_issue'], temp_df['model_issue'])\n",
    "    \n",
    "    agreement_results.append({\n",
    "        \"Model\": model,\n",
    "        \"Agreement with Consensus (Maint.)\": agree_maint,\n",
    "        \"Agreement with Consensus (Issue)\": agree_issue\n",
    "    })\n",
    "\n",
    "agreement_df = pd.DataFrame(agreement_results).set_index(\"Model\")\n",
    "display(agreement_df.style.format(\"{:.2%}\"))\n",
    "\n",
    "# --- Figure 8: Inter-Model Agreement Heatmap (Maintenance Type) ---\n",
    "print(\"\\nFigure 8: Inter-Model Agreement Heatmap (Maintenance Type)\")\n",
    "\n",
    "kappa_matrix = pd.DataFrame(index=model_names, columns=model_names, dtype=float)\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        \n",
    "        # If we are comparing a model to itself, the score is always a perfect 1.0\n",
    "        if model1 == model2:\n",
    "            kappa_matrix.loc[model1, model2] = 1.0\n",
    "            continue # Skip to the next comparison\n",
    "\n",
    "        # Create a temporary df and drop NaNs for the two different models\n",
    "        temp_kappa_df = master_df[[f\"{model1}_Maint_Type\", f\"{model2}_Maint_Type\"]].dropna()\n",
    "        \n",
    "        if not temp_kappa_df.empty:\n",
    "            # Force to string type AND extract the raw NumPy array with .values\n",
    "            y1 = temp_kappa_df[f\"{model1}_Maint_Type\"].astype(str).values\n",
    "            y2 = temp_kappa_df[f\"{model2}_Maint_Type\"].astype(str).values\n",
    "            \n",
    "            kappa_matrix.loc[model1, model2] = cohen_kappa_score(y1, y2)\n",
    "        else:\n",
    "            # If there's no overlap, the agreement score is undefined (NaN)\n",
    "            kappa_matrix.loc[model1, model2] = np.nan\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(kappa_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Pairwise Inter-Model Agreement (Cohen's Kappa for Maintenance Type)\")\n",
    "#plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_8.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Figure 9: Inter-Model Agreement Heatmap (Issue Category) ---\n",
    "print(\"\\nFigure 9: Inter-Model Agreement Heatmap (Issue Category)\")\n",
    "\n",
    "# Create a new DataFrame for the issue category results\n",
    "kappa_matrix_issue = pd.DataFrame(index=model_names, columns=model_names, dtype=float)\n",
    "\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        \n",
    "        if model1 == model2:\n",
    "            kappa_matrix_issue.loc[model1, model2] = 1.0\n",
    "            continue\n",
    "\n",
    "        # Select the '_Issue_Cat' columns instead of '_Maint_Type'\n",
    "        temp_kappa_df = master_df[[f\"{model1}_Issue_Cat\", f\"{model2}_Issue_Cat\"]].dropna()\n",
    "        \n",
    "        if not temp_kappa_df.empty:\n",
    "            y1 = temp_kappa_df[f\"{model1}_Issue_Cat\"].astype(str).values\n",
    "            y2 = temp_kappa_df[f\"{model2}_Issue_Cat\"].astype(str).values\n",
    "            \n",
    "            kappa_matrix_issue.loc[model1, model2] = cohen_kappa_score(y1, y2)\n",
    "        else:\n",
    "            kappa_matrix_issue.loc[model1, model2] = np.nan\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Use the new kappa_matrix_issue DataFrame and a different color map\n",
    "sns.heatmap(kappa_matrix_issue, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title(\"Pairwise Inter-Model Agreement (Cohen's Kappa for Issue Category)\")\n",
    "#plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_9.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Summary and Export ===\n",
    "\n",
    "print(\"INFO: Saving all generated tables...\")\n",
    "\n",
    "try:\n",
    "    # --- Save Tables ---\n",
    "    perf_summary_df.to_csv(ANALYSIS_OUTPUT_DIR / \"table_1_performance_summary.csv\")\n",
    "    \n",
    "    if run_accuracy_analysis:\n",
    "        accuracy_summary_df.to_csv(ANALYSIS_OUTPUT_DIR / \"table_2_accuracy_summary.csv\")\n",
    "        \n",
    "        # Check if the confidence analysis was run and the pivot tables exist\n",
    "        if 'conf_acc_pivot_maint' in locals():\n",
    "            conf_acc_pivot_maint.to_csv(ANALYSIS_OUTPUT_DIR / \"table_3_f1_vs_confidence_maint.csv\")\n",
    "        if 'conf_acc_pivot_issue' in locals():\n",
    "            conf_acc_pivot_issue.to_csv(ANALYSIS_OUTPUT_DIR / \"table_4_f1_vs_confidence_issue.csv\")\n",
    "\n",
    "    agreement_df.to_csv(ANALYSIS_OUTPUT_DIR / \"table_5_consensus_agreement.csv\")\n",
    "\n",
    "    print(f\"\\nAnalysis complete. All tables and figures saved to: {ANALYSIS_OUTPUT_DIR}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A DataFrame was not found, so not all tables could be saved. Please check for errors in previous cells. Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# === Addition: Visualisations used in the Academic Paper ===\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Generating Curated Tables and Figures for Publication ---\")\n",
    "\n",
    "# Define the desired, consistent order for models in all assets\n",
    "PAPER_MODEL_ORDER = [\n",
    "    'gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'o3', 'o4-mini', \n",
    "    'gemini-2.5-pro', 'gemini-2.5-flash', \n",
    "    'phi4_14b', 'gemma3_12b', 'llama3.1_8b', 'mistral_7b'\n",
    "]\n",
    "\n",
    "# --- 1. Final Summary Table: Overall Performance and Alignment ---\n",
    "print(\"\\n## Final Summary Table: Overall Performance and Alignment ##\")\n",
    "\n",
    "# Create a new DataFrame starting with the performance summary\n",
    "final_summary_df = perf_summary_df.copy()\n",
    "\n",
    "# Calculate Average F1 Score and add it\n",
    "if GOLDEN_DATASET in model_names and GOLDEN_DATASET not in accuracy_summary_df.index:\n",
    "    accuracy_summary_df.loc[GOLDEN_DATASET] = {\"Maint. Type F1\": 1.0, \"Issue Cat. F1\": 1.0}\n",
    "accuracy_summary_df['Avg F1 Score'] = accuracy_summary_df[['Maint. Type F1', 'Issue Cat. F1']].mean(axis=1)\n",
    "final_summary_df = final_summary_df.join(accuracy_summary_df[['Avg F1 Score']])\n",
    "\n",
    "# Calculate Average Consensus Agreement and add it\n",
    "agreement_df['Avg Consensus Agreement'] = agreement_df[['Agreement with Consensus (Maint.)', 'Agreement with Consensus (Issue)']].mean(axis=1)\n",
    "final_summary_df = final_summary_df.join(agreement_df[['Avg Consensus Agreement']])\n",
    "\n",
    "# Reorder and display the final table according to the specified order\n",
    "final_summary_df = final_summary_df.reindex(PAPER_MODEL_ORDER).dropna(how='all')\n",
    "\n",
    "display(final_summary_df.style.format({\n",
    "    \"Throughput (logs/s)\": \"{:.2f}\",\n",
    "    \"Estimated Cost ($)\": \"${:.2f}\",\n",
    "    \"Error Rate (%)\": \"{:.2f}%\",\n",
    "    \"Avg F1 Score\": \"{:.2f}\",\n",
    "    \"Avg Consensus Agreement\": \"{:.2%}\"\n",
    "}))\n",
    "# Save the final summary table\n",
    "final_summary_df.to_csv(ANALYSIS_OUTPUT_DIR / \"final_summary_table.csv\")\n",
    "\n",
    "\n",
    "# --- 2. Combined Chart: Throughput vs. Cost (Tornado Chart) ---\n",
    "print(\"\\n## Figure: Throughput vs. Cost Trade-off ##\")\n",
    "\n",
    "# Order by throughput (include ALL models)\n",
    "throughput_df = perf_summary_df.sort_values('Throughput (logs/s)')\n",
    "ordered_index = throughput_df.index\n",
    "\n",
    "# Align costs to the same order; zero for missing/locals\n",
    "cost_series = perf_summary_df['Estimated Cost ($)'].reindex(ordered_index).fillna(0)\n",
    "is_local = cost_series.eq(0)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12, 8), ncols=2, sharey=True)\n",
    "fig.suptitle('Throughput vs. Cost for API and Local Models', fontsize=18)\n",
    "\n",
    "# Left: Throughput (all models). Dim local (zero-cost) bars.\n",
    "left_bars = axes[0].barh(ordered_index, throughput_df['Throughput (logs/s)'], color='seagreen')\n",
    "axes[0].set_xlabel('Throughput (logs/s)')\n",
    "axes[0].invert_xaxis()\n",
    "\n",
    "for i, rect in enumerate(left_bars.patches):\n",
    "    if is_local.iloc[i]:\n",
    "        rect.set_alpha(0.4)\n",
    "\n",
    "# Right: Cost (aligned to same order)\n",
    "right_bars = axes[1].barh(ordered_index, cost_series, color='crimson')\n",
    "axes[1].set_xlabel('Estimated Cost ($)')\n",
    "\n",
    "for i, rect in enumerate(right_bars.patches):\n",
    "    if is_local.iloc[i]:\n",
    "        rect.set_alpha(0.4)\n",
    "        rect.set_hatch('///')\n",
    "        rect.set_edgecolor('dimgray')\n",
    "        rect.set_linewidth(0.5)\n",
    "\n",
    "\n",
    "# Invert y on the left axis so both share the same inverted order\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_1.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. F1-Score Comparison Bar Chart (Sorted by Average, gpt-5 excluded) ---\n",
    "print(\"\\n## Figure: F1-Score Comparison (Sorted by Average) ##\")\n",
    "plot_df_f1 = accuracy_summary_df.drop(index=GOLDEN_DATASET, errors='ignore')\n",
    "plot_df_f1_sorted = plot_df_f1.sort_values('Avg F1 Score', ascending=False)\n",
    "\n",
    "plot_df_f1_sorted[['Maint. Type F1', 'Issue Cat. F1']].plot(kind='bar', figsize=(14, 7))\n",
    "plt.title(f\"F1-Score for Alignment with {ground_truth_source} (Sorted by Average)\")\n",
    "plt.ylabel(\"Weighted F1-Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Master Chart for F1-Score vs. Confidence ---\n",
    "print(\"\\n## Figure: Model Calibration: Average F1-Score vs. Confidence ##\")\n",
    "\n",
    "# Recompute per-confidence F1 specifically for this plot, INCLUDING the GOLDEN_DATASET\n",
    "conf_rows = []\n",
    "for model in model_names:\n",
    "    for level in ['High', 'Medium', 'Low']:\n",
    "        col_conf = f\"{model}_Certainty\"\n",
    "        if col_conf not in master_df.columns:\n",
    "            continue\n",
    "        subset_df = master_df[master_df[col_conf] == level]\n",
    "        if subset_df.empty:\n",
    "            continue\n",
    "\n",
    "        df_maint = subset_df.dropna(subset=['Golden_Maint_Type', f\"{model}_Maint_Type\"])\n",
    "        df_issue = subset_df.dropna(subset=['Golden_Issue_Cat', f\"{model}_Issue_Cat\"])\n",
    "        if df_maint.empty or df_issue.empty:\n",
    "            continue\n",
    "\n",
    "        f1_maint = f1_score(df_maint['Golden_Maint_Type'], df_maint[f\"{model}_Maint_Type\"], average='weighted', zero_division=0)\n",
    "        f1_issue = f1_score(df_issue['Golden_Issue_Cat'], df_issue[f\"{model}_Issue_Cat\"], average='weighted', zero_division=0)\n",
    "        conf_rows.append({\n",
    "            \"Model\": model,\n",
    "            \"Confidence\": level,\n",
    "            \"Avg F1 Score\": float(np.mean([f1_maint, f1_issue])),\n",
    "        })\n",
    "\n",
    "conf_acc_df_plot = pd.DataFrame(conf_rows)\n",
    "\n",
    "if not conf_acc_df_plot.empty:\n",
    "    conf_pivot_avg = conf_acc_df_plot.pivot(index='Model', columns='Confidence', values='Avg F1 Score')\n",
    "\n",
    "    # Ensure desired column order\n",
    "    desired_order = ['High', 'Medium', 'Low']\n",
    "    conf_pivot_avg = conf_pivot_avg.reindex(columns=[c for c in desired_order if c in conf_pivot_avg.columns])\n",
    "\n",
    "    # Sort by 'High' if available, else by the first available column\n",
    "    sort_col = 'High' if 'High' in conf_pivot_avg.columns else conf_pivot_avg.columns[0]\n",
    "    conf_pivot_avg = conf_pivot_avg.sort_values(sort_col, ascending=False)\n",
    "\n",
    "    # Explicit colors per confidence\n",
    "    confidence_colors = {\n",
    "        'High':   '#1f77b4',  # blue\n",
    "        'Medium': '#2ca02c',  # green\n",
    "        'Low':    '#ff7f0e',  # orange\n",
    "    }\n",
    "\n",
    "    ax = conf_pivot_avg.plot(\n",
    "        kind='bar', figsize=(14, 7),\n",
    "        color=[confidence_colors[c] for c in conf_pivot_avg.columns]\n",
    "    )\n",
    "\n",
    "    # Dim only the GOLDEN_DATASET bars (now present)\n",
    "    if GOLDEN_DATASET in conf_pivot_avg.index:\n",
    "        gpt5_index = conf_pivot_avg.index.get_loc(GOLDEN_DATASET)\n",
    "        for container in ax.containers:  # one BarContainer per confidence level\n",
    "            if gpt5_index < len(container.patches):\n",
    "                container.patches[gpt5_index].set_alpha(0.4)\n",
    "\n",
    "    # Legend with full-opacity proxy patches using the same mapping\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_handles = [\n",
    "        Patch(facecolor=confidence_colors[label], label=label, alpha=1.0)\n",
    "        for label in conf_pivot_avg.columns\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, title=\"Confidence Level\")\n",
    "\n",
    "    plt.title(f\"Model Calibration: Average F1-Score vs. Confidence\")\n",
    "    plt.ylabel(\"Average Weighted F1-Score\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_3.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"INFO: Confidence vs. F1-Score data not available to plot.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. Confusion Matrix (Top Model, No Labels, Bigger Squares) ---\n",
    "print(\"\\n## Figure: Confusion Matrix Pattern (Top Model, Issue Category) ##\")\n",
    "if not accuracy_summary_df.drop(index=GOLDEN_DATASET, errors='ignore').empty:\n",
    "    top_model = accuracy_summary_df.drop(index=GOLDEN_DATASET, errors='ignore').sort_values('Issue Cat. F1', ascending=False).index[0]\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    temp_df_issue = master_df[['Golden_Issue_Cat', f\"{top_model}_Issue_Cat\"]].dropna()\n",
    "    issue_labels = pd.unique(temp_df_issue[['Golden_Issue_Cat', f\"{top_model}_Issue_Cat\"]].values.ravel('K'))\n",
    "    cm_issue = confusion_matrix(temp_df_issue['Golden_Issue_Cat'], temp_df_issue[f\"{top_model}_Issue_Cat\"], labels=issue_labels)\n",
    "    sns.heatmap(cm_issue, annot=True, fmt='d', cmap='Reds', xticklabels=False, yticklabels=False, annot_kws={\"size\": 8})\n",
    "    plt.title(f\"Confusion Matrix Pattern for Issue Category Labels Generated by '{top_model}'\")\n",
    "    plt.ylabel(\"Refernce Label\")\n",
    "    plt.xlabel(\"Generated Label\")\n",
    "    plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_4.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"INFO: No accuracy data available to generate confusion matrix.\")\n",
    "\n",
    "\n",
    "# --- 6. Averaged Kappa Heatmap ---\n",
    "print(\"\\n## Figure: Averaged Inter-Model Agreement Heatmap ##\")\n",
    "if 'kappa_matrix' in locals() and 'kappa_matrix_issue' in locals():\n",
    "    kappa_matrix_avg = (kappa_matrix + kappa_matrix_issue) / 2\n",
    "    \n",
    "    kappa_matrix_sorted = kappa_matrix_avg.reindex(index=PAPER_MODEL_ORDER, columns=PAPER_MODEL_ORDER).dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(kappa_matrix_sorted, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(\"Average Pairwise Inter-Model Agreement (Cohen's Kappa)\")\n",
    "    plt.savefig(ANALYSIS_OUTPUT_DIR / \"figure_5.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"INFO: Kappa matrices not available to generate averaged heatmap.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
