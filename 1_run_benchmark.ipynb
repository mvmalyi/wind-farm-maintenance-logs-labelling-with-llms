{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1_run_benchmark.ipynb\n",
    "#\n",
    "# This notebook orchestrates the entire LLM benchmarking process. \n",
    "# It loads the data and configuration, iterates through each model\n",
    "# and log entry, calls the respective LLM APIs, validates the\n",
    "# responses, and saves the results for analysis.\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65643e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. SETUP AND IMPORTS ===\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Custom Modules ---\n",
    "from llm_clients.openai_client import OpenAIClient\n",
    "from llm_clients.google_client import GoogleClient\n",
    "from llm_clients.ollama_client import OllamaClient\n",
    "from llm_clients.base_client import ModelNotFoundError\n",
    "from utils.data_schemas import LLMOutput\n",
    "from utils.logging_config import setup_logging\n",
    "\n",
    "print(\"INFO: Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. RUN CONFIGURATION ===\n",
    "# Set to an integer (e.g., 50) for a test run on the first n rows.\n",
    "# Set to None to process the entire dataset.\n",
    "ROWS_TO_PROCESS = 50\n",
    "\n",
    "CONFIG_FILE = \"configurations/config.yaml\"\n",
    "PROMPTS_FILE = \"prompts.json\" \n",
    "\n",
    "print(\"INFO: The configuration file and prompts have been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e02ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. HELPER FUNCTIONS ===\n",
    "\n",
    "def load_config(config_path=\"config.yaml\"):\n",
    "    \"\"\"Loads the YAML configuration file.\"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def get_api_key(key_name: str, config: dict) -> str:\n",
    "    \"\"\"Retrieves an API key using a 3-step check.\"\"\"\n",
    "    env_var = key_name.upper()\n",
    "    api_key = os.getenv(env_var)\n",
    "    if api_key:\n",
    "        logging.info(f\"Found API key in environment variable '{env_var}'.\")\n",
    "        return api_key\n",
    "    \n",
    "    api_key = config.get(key_name)\n",
    "    if api_key:\n",
    "        logging.info(f\"Found API key in config.yaml.\")\n",
    "        return api_key\n",
    "\n",
    "    logging.warning(f\"API key for '{key_name}' not found as an environment variable or in the config file. \"\n",
    "        \"Waiting for manual entry...\")\n",
    "    api_key = getpass.getpass(f\"Please enter your {key_name}: \")\n",
    "    return api_key\n",
    "\n",
    "def build_lookup_map(prompts_config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Builds a fast lookup map from component code to a list of valid labels,\n",
    "    supporting both 'include' (list) and 'exclude' (dict) rules given in the prompts JSON.\n",
    "    \"\"\"\n",
    "    logging.info(\"Building component-to-label lookup map with include/exclude logic...\")\n",
    "    \n",
    "    all_codes = set()\n",
    "    label_groups = [prompts_config[\"maintenance_types\"], prompts_config[\"issue_categories\"]]\n",
    "\n",
    "    # --- 1. First, gather all unique component codes from the entire file ---\n",
    "    for group in label_groups:\n",
    "        for value in group.values():\n",
    "            if isinstance(value, list) and \"ANY\" not in value:\n",
    "                all_codes.update(value)\n",
    "\n",
    "    # --- 2. Build the map using the new rules ---\n",
    "    lookup_map = defaultdict(lambda: {\"maintenance\": [], \"issue\": []})\n",
    "\n",
    "    # --- 3. Process Maintenance Types ---\n",
    "    for maint_type, value in prompts_config[\"maintenance_types\"].items():\n",
    "        if isinstance(value, list): # The rule for including whatever component codes are listed\n",
    "            if \"ANY\" in value:\n",
    "                for code in all_codes:\n",
    "                    lookup_map[code][\"maintenance\"].append(maint_type)\n",
    "            else:\n",
    "                for code in value:\n",
    "                    lookup_map[code][\"maintenance\"].append(maint_type)\n",
    "        elif isinstance(value, dict) and \"exclude_codes\" in value: # The rule for excluding whatever component codes are in a dictionary\n",
    "            codes_to_exclude = set(value[\"exclude_codes\"])\n",
    "            codes_to_include = all_codes - codes_to_exclude\n",
    "            for code in codes_to_include:\n",
    "                lookup_map[code][\"maintenance\"].append(maint_type)\n",
    "\n",
    "    # --- 4. Process Issue Categories ---\n",
    "    for issue_cat, value in prompts_config[\"issue_categories\"].items():\n",
    "        if isinstance(value, list): # rule for the included component codes\n",
    "            if \"ANY\" in value:\n",
    "                for code in all_codes:\n",
    "                    lookup_map[code][\"issue\"].append(issue_cat)\n",
    "            else:\n",
    "                for code in value:\n",
    "                    lookup_map[code][\"issue\"].append(issue_cat)\n",
    "        elif isinstance(value, dict) and \"exclude_codes\" in value: # rule for the excluded component codes\n",
    "            codes_to_exclude = set(value[\"exclude_codes\"])\n",
    "            codes_to_include = all_codes - codes_to_exclude\n",
    "            for code in codes_to_include:\n",
    "                lookup_map[code][\"issue\"].append(issue_cat)\n",
    "\n",
    "    logging.info(\"Lookup map built successfully.\")\n",
    "    return dict(lookup_map)\n",
    "\n",
    "def update_cost_log(model_config: dict, run_prompt_tokens: int, run_completion_tokens: int, cost_log_dir: str):\n",
    "    \"\"\"\n",
    "    Calculates the cost of the current run and updates a persistent\n",
    "    JSON log file with cumulative token and cost data for a model.\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    logging.info(f\"Updating cost log for {model_name}...\")\n",
    "    \n",
    "    # --- 1. Calculate cost for the current run ---\n",
    "    # 'individual' pricing is used for now, as batching is not implemented\n",
    "    pricing = model_config.get(\"pricing\", {}).get(\"individual\", {})\n",
    "    price_input = pricing.get(\"input\", 0.0)\n",
    "    price_output = pricing.get(\"output\", 0.0)\n",
    "    \n",
    "    run_cost = ((run_prompt_tokens / 1_000_000) * price_input) + \\\n",
    "               ((run_completion_tokens / 1_000_000) * price_output)\n",
    "\n",
    "    # --- 2. Read existing cumulative data ---\n",
    "    cost_log_dir_path = Path(cost_log_dir)\n",
    "    cost_log_dir_path.mkdir(exist_ok=True)\n",
    "    log_filename = f\"{model_name.replace(':', '_').replace('/', '_')}_cost_log.json\"\n",
    "    log_filepath = cost_log_dir_path / log_filename\n",
    "    \n",
    "    cumulative_data = {\n",
    "        \"cumulative_prompt_tokens\": 0,\n",
    "        \"cumulative_completion_tokens\": 0,\n",
    "        \"cumulative_cost_usd\": 0.0\n",
    "    }\n",
    "    \n",
    "    if log_filepath.exists():\n",
    "        with open(log_filepath, \"r\") as f:\n",
    "            try:\n",
    "                cumulative_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                logging.warning(f\"Could not read cost log file for {model_name}. Starting from zero.\")\n",
    "\n",
    "    # --- 3. Update cumulative totals ---\n",
    "    cumulative_data[\"cumulative_prompt_tokens\"] += run_prompt_tokens\n",
    "    cumulative_data[\"cumulative_completion_tokens\"] += run_completion_tokens\n",
    "    cumulative_data[\"cumulative_cost_usd\"] += run_cost\n",
    "    cumulative_data[\"model_name\"] = model_name\n",
    "    cumulative_data[\"last_updated\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # --- 4. Write new data back to the file ---\n",
    "    with open(log_filepath, \"w\") as f:\n",
    "        json.dump(cumulative_data, f, indent=4)\n",
    "\n",
    "    logging.info(f\"Cost for this run ({model_name}): ${run_cost:.6f}\")\n",
    "    logging.info(f\"New cumulative cost for {model_name}: ${cumulative_data['cumulative_cost_usd']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. INITIALISATION ===\n",
    "\n",
    "# --- Load Config ---\n",
    "try:\n",
    "    config = load_config(CONFIG_FILE)\n",
    "    print(f\"INFO: Configuration loaded from '{CONFIG_FILE}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Configuration file '{CONFIG_FILE}' not found. Aborting.\")\n",
    "    raise\n",
    "\n",
    "# --- Set up dynamic paths for the current run ---\n",
    "# 1. Create a unique ID for this run using a timestamp.\n",
    "run_id = f\"run_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"INFO: Initialising Run ID: {run_id}\")\n",
    "\n",
    "# 2. Define run-specific output directories.\n",
    "run_dir = Path(\"outputs\") / \"runs\" / run_id\n",
    "individual_dir_run = run_dir / \"individual_results\"\n",
    "final_dir_run = run_dir \n",
    "log_dir_run = run_dir\n",
    "\n",
    "# 3. Create these new directories.\n",
    "individual_dir_run.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4. Update the config dictionary in memory to use these new paths.\n",
    "config['file_paths']['log_dir'] = str(log_dir_run)\n",
    "config['file_paths']['individual_results_dir'] = str(individual_dir_run)\n",
    "config['file_paths']['final_results_dir'] = str(final_dir_run)\n",
    "\n",
    "# --- Set up logging to go into the new run-specific directory ---\n",
    "setup_logging(config[\"file_paths\"][\"log_dir\"])\n",
    "\n",
    "# --- Suppress verbose logs from external libraries ---\n",
    "# 1. This silences the low-level HTTP request logs.\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "# 2. This silences the main OpenAI library's info logs.\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "\n",
    "# --- Load Prompts and Build Lookup Map ---\n",
    "try:\n",
    "    with open(PROMPTS_FILE, \"r\") as f:\n",
    "        prompts_config = json.load(f)\n",
    "    component_label_map = build_lookup_map(prompts_config)\n",
    "    prompt_template = prompts_config[\"prompt_template\"]\n",
    "    logging.info(f\"Prompts loaded and processed from '{PROMPTS_FILE}'.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Prompts file '{PROMPTS_FILE}' not found. Aborting.\")\n",
    "    raise\n",
    "    \n",
    "# --- Load and Slice Input Data ---\n",
    "try:\n",
    "    input_df_full = pd.read_csv(config[\"file_paths\"][\"input_data\"])\n",
    "    if ROWS_TO_PROCESS:\n",
    "        input_df = input_df_full.head(ROWS_TO_PROCESS).copy()\n",
    "        logging.info(f\"Loaded and sliced input data: Processing first {ROWS_TO_PROCESS} rows.\")\n",
    "    else:\n",
    "        input_df = input_df_full.copy()\n",
    "        logging.info(f\"Loaded input data: Processing all {len(input_df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Input data file not found at {config['file_paths']['input_data']}. Aborting.\")\n",
    "    raise\n",
    "\n",
    "# --- Initialise API Keys & Clients ---\n",
    "api_keys_cache = {}\n",
    "clients = {\"openai\": OpenAIClient, \"google\": GoogleClient, \"ollama\": OllamaClient}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. MAIN BENCHMARKING LOOP ===\n",
    "\n",
    "logging.info(\"Starting the main benchmarking process...\")\n",
    "Path(config[\"file_paths\"][\"individual_results_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "for model_config in config[\"models_to_benchmark\"]:\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    client_type = model_config[\"client\"]\n",
    "    logging.info(f\"===== Processing Model: {model_name} =====\")\n",
    "    \n",
    "    # --- Initialise Client ---\n",
    "    client_class = clients.get(client_type)\n",
    "    if not client_class:\n",
    "        logging.warning(f\"No client found for type '{client_type}'. Skipping model {model_name}.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        if client_type == \"ollama\":\n",
    "            client = client_class(model_config, config[\"api_settings\"][\"ollama_endpoint\"])\n",
    "        else:\n",
    "            # --- API key retrieval ---\n",
    "            # Check if we've already fetched this key during this run\n",
    "            if client_type not in api_keys_cache:\n",
    "                logging.info(f\"API key for '{client_type}' not yet loaded. Retrieving now...\")\n",
    "                key_name = f\"{client_type}_api_key\"\n",
    "                # Fetch the key and store it in our cache for this run\n",
    "                api_keys_cache[client_type] = get_api_key(key_name, config[\"api_settings\"])\n",
    "            \n",
    "            # Use the key from the cache to initialise the client\n",
    "            api_key = api_keys_cache.get(client_type)\n",
    "            if not api_key:\n",
    "                logging.error(f\"Failed to retrieve API key for {client_type}. Skipping model {model_name}.\")\n",
    "                continue\n",
    "            client = client_class(model_config, api_key)\n",
    "    \n",
    "    except ModelNotFoundError as e:\n",
    "        # This will catch the error from the Ollama client's pre-flight check\n",
    "        logging.warning(f\"Skipping model '{model_name}': {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Loop through DataFrame ---\n",
    "    model_results = []\n",
    "    start_time = time.time()\n",
    "    for index, row in tqdm(input_df.iterrows(), total=len(input_df), desc=f\"Model: {model_name}\"):\n",
    "        # --- a. Extract and Prepare Data ---\n",
    "        wonum, component_name, component_code = row.get(\"WONUM\"), row.get(\"Component Name\"), row.get(\"Component Code\")\n",
    "        description = row.get(\"Description_EN\") if model_config.get(\"requires_translation\") else row.get(\"Description\")\n",
    "        observations = row.get(\"Observations_EN\") if model_config.get(\"requires_translation\") else row.get(\"Observations\")\n",
    "        observations = \"\" if pd.isna(observations) else observations\n",
    "\n",
    "        # --- b. Construct Dynamic Prompt ---\n",
    "        valid_labels = component_label_map.get(component_code, {})\n",
    "        \n",
    "        # Fallback to an empty list if a component has no specific labels\n",
    "        maint_types_list = valid_labels.get(\"maintenance\", [])\n",
    "        issue_cats_list = valid_labels.get(\"issue\", [])\n",
    "        \n",
    "        # Ensure lists are not empty; if so, could use all labels as a fallback (optional)\n",
    "        if not maint_types_list: logging.warning(f\"WONUM {wonum}: No specific maintenance types found for {component_code}.\")\n",
    "        if not issue_cats_list: logging.warning(f\"WONUM {wonum}: No specific issue categories found for {component_code}.\")\n",
    "\n",
    "        formatted_maint_types = \"\\n\".join([f\"- {t}\" for t in sorted(maint_types_list)])\n",
    "        formatted_issue_cats = \"\\n\".join([f\"- {c}\" for c in sorted(issue_cats_list)])\n",
    "\n",
    "        prompt = prompt_template.format(\n",
    "            component_name=component_name, description=description, observations=observations,\n",
    "            maintenance_types=formatted_maint_types, issue_categories=formatted_issue_cats\n",
    "        )\n",
    "\n",
    "        # --- c. Get Completion ---\n",
    "        client_response = client.get_completion(prompt)\n",
    "        \n",
    "        # --- d. Parse and Validate ---\n",
    "        result_record = {\n",
    "            \"WONUM\": wonum,\n",
    "            \"Model Name\": model_name,\n",
    "            \"Prompt Tokens\": client_response.prompt_tokens,\n",
    "            \"Completion Tokens\": client_response.completion_tokens,\n",
    "            \"Sleep Duration\": client_response.sleep_duration\n",
    "        }\n",
    "        if client_response.error:\n",
    "            logging.error(f\"WONUM {wonum}: Client error for {model_name}: {client_response.error}\")\n",
    "            result_record.update({\"Maintenance Type\": \"CLIENT_ERROR\", \"Issue Category\": \"CLIENT_ERROR\", \"Specific Problem\": client_response.error, \"Certainty Level\": \"Unknown\"})\n",
    "        else:\n",
    "            try:\n",
    "                validated_output = LLMOutput.model_validate_json(client_response.content)\n",
    "                result_record.update(validated_output.model_dump(by_alias=True))\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"WONUM {wonum}: Pydantic validation failed for {model_name}. Error: {e}\")\n",
    "                result_record.update({\"Maintenance Type\": \"FORMATTING_ERROR\", \"Issue Category\": \"FORMATTING_ERROR\", \"Specific Problem\": str(e), \"Certainty Level\": \"Unknown\"})\n",
    "        \n",
    "        model_results.append(result_record)\n",
    "\n",
    "    # --- e. Log Performance and Save Results ---\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # --- 'Actual Performance' Calculation for Google models ---\n",
    "    # Sum the sleep durations for this model's run\n",
    "    total_sleep_time = sum(res.get(\"Sleep Duration\", 0.0) for res in model_results)\n",
    "    actual_processing_time = total_time - total_sleep_time\n",
    "    \n",
    "    # Calculate performance based on actual processing time\n",
    "    logs_per_second_actual = len(input_df) / actual_processing_time if actual_processing_time > 0 else 0\n",
    "\n",
    "    logging.info(f\"===== Finished Model: {model_name} =====\")\n",
    "    logging.info(f\"Total Wall-Clock Time: {total_time:.2f}s (includes sleep).\")\n",
    "    if client_type == 'google':\n",
    "        logging.info(f\"Total Time Slept: {total_sleep_time:.2f}s.\")\n",
    "        logging.info(f\"Actual Processing Time: {actual_processing_time:.2f}s.\")\n",
    "        logging.info(f\"Actual Performance: {logs_per_second_actual:.2f} logs/second.\")\n",
    "    else:\n",
    "        logging.info(f\"Performance: {logs_per_second_actual:.2f} logs/second.\")\n",
    "    \n",
    "    # --- Update Performance Log CSV ---\n",
    "    try:\n",
    "        performance_log_path = Path(\"outputs/performance_log.csv\")\n",
    "        performance_data = {\n",
    "            \"Timestamp\": [time.strftime(\"%Y-%m-%d %H:%M:%S\")], \"Model\": [model_name],\n",
    "            \"Logs Processed\": [len(input_df)], \"Total Time (s)\": [round(total_time, 2)],\n",
    "            \"Actual Time (s)\": [round(actual_processing_time, 2)], \n",
    "            \"Performance (logs/s)\": [round(logs_per_second_actual, 2)] \n",
    "        }\n",
    "        perf_df = pd.DataFrame(performance_data)\n",
    "        write_header = not performance_log_path.exists()\n",
    "        \n",
    "        # If the file exists and our new column isn't there, we should rewrite it\n",
    "        if not write_header:\n",
    "            existing_df = pd.read_csv(performance_log_path)\n",
    "            if \"Actual Time (s)\" not in existing_df.columns:\n",
    "                write_header = True # Force rewrite with new header\n",
    "\n",
    "        perf_df.to_csv(performance_log_path, mode='w' if write_header else 'a', header=write_header, index=False)\n",
    "        logging.info(f\"Performance metrics for {model_name} saved to {performance_log_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write to performance log: {e}\")\n",
    "\n",
    "    # --- Save Individual Model Results ---\n",
    "    # We also need to add the sleep duration to the individual results\n",
    "    for i, res in enumerate(model_results):\n",
    "        input_df.loc[input_df.index[i], f\"{model_name.replace(':', '_')}_Sleep_Duration\"] = res.get(\"Sleep Duration\", 0.0)\n",
    "    \n",
    "    # Rebuild results_df from our list of dicts which is cleaner\n",
    "    final_model_results = []\n",
    "    for res in model_results:\n",
    "        # We need to add the WONUM to each dict before creating the DataFrame\n",
    "        # This assumes your original `model_results` loop is creating dicts with WONUM\n",
    "        final_model_results.append(res)\n",
    "        \n",
    "    results_df = pd.DataFrame(final_model_results)\n",
    "    \n",
    "    # Rename columns before saving\n",
    "    model_name_suffix = model_name.replace(':', '_').replace('/', '_')\n",
    "    results_df = results_df.rename(columns={\n",
    "        \"Maintenance Type\": f\"{model_name_suffix}_Maint_Type\",\n",
    "        \"Issue Category\": f\"{model_name_suffix}_Issue_Cat\",\n",
    "        \"Specific Problem\": f\"{model_name_suffix}_Spec_Problem\",\n",
    "        \"Certainty Level\": f\"{model_name_suffix}_Certainty\",\n",
    "        \"Prompt Tokens\": f\"{model_name_suffix}_Prompt_Tokens\",\n",
    "        \"Completion Tokens\": f\"{model_name_suffix}_Comp_Tokens\",\n",
    "        \"Sleep Duration\": f\"{model_name_suffix}_Sleep_Duration\"\n",
    "    }).drop(columns=[\"Model Name\"])\n",
    "\n",
    "    output_filename = f\"{model_name_suffix}_results.csv\"\n",
    "    output_path = Path(config[\"file_paths\"][\"individual_results_dir\"]) / output_filename\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Results for {model_name} saved to {output_path}\")\n",
    "\n",
    "    # --- f. Update Cost Log (if applicable) ---\n",
    "    if client_type in ['openai', 'google']:\n",
    "        # Sum the tokens used in this specific run and convert to standard Python integers\n",
    "        total_prompt_tokens_run = int(results_df[f\"{model_name_suffix}_Prompt_Tokens\"].sum())\n",
    "        total_completion_tokens_run = int(results_df[f\"{model_name_suffix}_Comp_Tokens\"].sum())\n",
    "        \n",
    "        # Call the helper function to update the persistent log\n",
    "        update_cost_log(\n",
    "            model_config=model_config,\n",
    "            run_prompt_tokens=total_prompt_tokens_run,\n",
    "            run_completion_tokens=total_completion_tokens_run,\n",
    "            cost_log_dir=\"outputs/cost_tracking\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1509031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. FINAL COMPILATION ===\n",
    "# This step combines all individual results into a single master file for easier analysis.\n",
    "logging.info(\"Compiling all individual results into a master file...\")\n",
    "\n",
    "individual_dir = Path(config[\"file_paths\"][\"individual_results_dir\"])\n",
    "all_dfs = []\n",
    "\n",
    "# Check if the directory exists and has files, to prevent errors on a clean run\n",
    "if individual_dir.exists():\n",
    "    for csv_file in individual_dir.glob(\"*_results.csv\"):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "if all_dfs:\n",
    "    # Merge the individual results with the original input data on WONUM\n",
    "    master_df = input_df.copy()\n",
    "    \n",
    "    # Reload the individual CSVs to merge them\n",
    "    for csv_file in individual_dir.glob(\"*_results.csv\"):\n",
    "        res_df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # --- Get the model name from the filename ---\n",
    "        # e.g., \"gpt-5_results.csv\" -> \"gpt-5_results\" -> \"gpt-5\"\n",
    "        model_name_suffix = csv_file.stem.replace('_results', '')\n",
    "        \n",
    "        # Rename the columns from the loaded file\n",
    "        res_df = res_df.rename(columns={\n",
    "            \"Maintenance Type\": f\"{model_name_suffix}_Maint_Type\",\n",
    "            \"Issue Category\": f\"{model_name_suffix}_Issue_Cat\",\n",
    "            \"Specific Problem\": f\"{model_name_suffix}_Spec_Problem\",\n",
    "            \"Certainty Level\": f\"{model_name_suffix}_Certainty\",\n",
    "            \"Prompt Tokens\": f\"{model_name_suffix}_Prompt_Tokens\",\n",
    "            \"Completion Tokens\": f\"{model_name_suffix}_Comp_Tokens\",\n",
    "            \"Sleep Duration\": f\"{model_name_suffix}_Sleep_Duration\"\n",
    "        })\n",
    "        \n",
    "        # Merge this model's data into the master DataFrame\n",
    "        master_df = pd.merge(master_df, res_df, on=\"WONUM\", how=\"left\")\n",
    "\n",
    "    Path(config[\"file_paths\"][\"final_results_dir\"]).mkdir(exist_ok=True)\n",
    "    master_output_path = Path(config[\"file_paths\"][\"final_results_dir\"]) / \"Master_Benchmark_Results.csv\"\n",
    "    master_df.to_csv(master_output_path, index=False)\n",
    "    logging.info(f\"Master results file saved to {master_output_path}\")\n",
    "else:\n",
    "    logging.warning(\"No individual result files found to compile.\")\n",
    "\n",
    "logging.info(\"Benchmark run complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCADA_Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
