{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1277accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "import umap\n",
    "import warnings\n",
    "\n",
    "# --- Global Configuration ---\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# --- Parameters for Cleaning --- \n",
    "INFRASTRUCTURE_LOG_PERCENT = 0.05\n",
    "PREFIX_SUFFIX_N_CHARS = 10\n",
    "SIMILARITY_THRESHOLD_HIGH = 0.90\n",
    "OBSERVATION_STRING = \"Nenhuma observação fornecida\" # Portuguese for \"No observation provided\", replace with what's relevant for your dataset\n",
    "NA_OBSERVATION_TARGET_PERCENT = 0.01\n",
    "\n",
    "# --- Parameters for Frequency Balancing ---\n",
    "DUPLICATE_LIMIT_TRIO = 3\n",
    "DUPLICATE_LIMIT_PAIR = 6\n",
    "DUPLICATE_LIMIT_DESC = 9\n",
    "DUPLICATE_LIMIT_OBS = 9\n",
    "\n",
    "# --- Parameters for 'Informative' Dataset ---\n",
    "WORD_COUNT_THRESHOLD = 3\n",
    "DISSIMILARITY_THRESHOLD = 0.6\n",
    "\n",
    "# --- File Paths ---\n",
    "INPUT_FILE = 'preliminary_cleaned_data.csv'\n",
    "CLEANED_OUTPUT_FILE = 'updated_cleaned_data.csv'\n",
    "INFORMATIVE_OUTPUT_FILE = 'informative_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40dad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# --- Helper Functions ---\n",
    "# =====================================================================================\n",
    "\n",
    "def initial_filter_and_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs initial filtering, component consolidation, and regex cleaning.\"\"\"\n",
    "    print(\"--- 2. Initial Filtering & Cleaning ---\")\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Limit infrastructure logs\n",
    "    null_turbine_indices = df_copy.index[df_copy['Turbine No'].isnull()].tolist()\n",
    "    if len(null_turbine_indices) > (df_copy.shape[0] * INFRASTRUCTURE_LOG_PERCENT):\n",
    "        target_keep = int(df_copy.shape[0] * INFRASTRUCTURE_LOG_PERCENT)\n",
    "        to_drop_count = len(null_turbine_indices) - target_keep\n",
    "        indices_to_drop = np.random.choice(null_turbine_indices, size=to_drop_count, replace=False)\n",
    "        df_copy.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "    # Filter for Component Name\n",
    "    df_copy.dropna(subset=['Component Name'], inplace=True)\n",
    "    df_copy = df_copy[df_copy['Component Name'] != '']\n",
    "    \n",
    "    # Consolidate Rotor Blade names\n",
    "    component_name_map = {\n",
    "        'Rotor Blade System 1': 'Rotor Blade System', 'Rotor Blade System 2': 'Rotor Blade System',\n",
    "        'Rotor Blade System 3': 'Rotor Blade System', 'Rotor Blades Overall': 'Rotor Blade System'\n",
    "    }\n",
    "    df_copy['Component Name'] = df_copy['Component Name'].replace(component_name_map)\n",
    "\n",
    "    # Regex cleaning\n",
    "    wo_pattern = r'\\(WO\\s\\d+\\)'\n",
    "    log_code_pattern = r'\\b\\d{2}\\.\\d{3}\\.\\d{2}-\\d{4}\\b'\n",
    "    multiple_spaces_pattern = r'\\s+'\n",
    "    para_pattern = r'\\s+para$'\n",
    "    for col in ['Description', 'Observations']:\n",
    "        df_copy[col] = (df_copy[col].astype(str)\n",
    "                        .str.replace(wo_pattern, '', regex=True)\n",
    "                        .str.replace(log_code_pattern, '', regex=True)\n",
    "                        .str.replace(multiple_spaces_pattern, ' ', regex=True)\n",
    "                        .str.strip())\n",
    "    df_copy['Observations'] = df_copy['Observations'].str.replace(para_pattern, '', regex=True).str.strip()\n",
    "    \n",
    "    print(\"Initial filtering and cleaning complete.\\n\")\n",
    "    return df_copy\n",
    "\n",
    "def remove_redundancy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies a cascade of filters to remove redundant rows.\"\"\"\n",
    "    print(\"--- 3. Removing Redundant Rows ---\")\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Exact matches\n",
    "    df_copy = df_copy[df_copy['Description'] != df_copy['Observations']]\n",
    "\n",
    "    # Prefix/Suffix matches\n",
    "    def is_prefix_suffix_redundant(row, n):\n",
    "        desc, obs = row['Description'], row['Observations']\n",
    "        if not isinstance(desc, str) or not isinstance(obs, str): return False\n",
    "        if obs.startswith(desc) and len(obs) <= len(desc) + n: return True\n",
    "        if obs.endswith(desc) and len(obs) <= len(desc) + n: return True\n",
    "        return False\n",
    "    redundant_mask = df_copy.apply(is_prefix_suffix_redundant, n=PREFIX_SUFFIX_N_CHARS, axis=1)\n",
    "    df_copy = df_copy[~redundant_mask]\n",
    "    \n",
    "    # High-similarity matches\n",
    "    def calculate_similarity(a, b):\n",
    "        if not isinstance(a, str) or not isinstance(b, str): return 0.0\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    df_copy['similarity'] = df_copy.apply(lambda row: calculate_similarity(row['Description'], row['Observations']), axis=1)\n",
    "    df_copy = df_copy[df_copy['similarity'] <= SIMILARITY_THRESHOLD_HIGH]\n",
    "    df_copy.drop(columns=['similarity'], inplace=True)\n",
    "    \n",
    "    print(\"Redundancy removal complete.\\n\")\n",
    "    return df_copy\n",
    "\n",
    "def balance_frequencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Balances component and text frequencies through down-sampling and duplicate capping.\"\"\"\n",
    "    print(\"--- 4. Frequency Balancing & Final Content Filtering ---\")\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Down-sample the majority component class\n",
    "    component_counts = df_copy['Component Name'].value_counts()\n",
    "    if len(component_counts) > 1:\n",
    "        majority_class_name = component_counts.index[0]\n",
    "        cap_limit = component_counts.iloc[1]\n",
    "        df_majority = df_copy[df_copy['Component Name'] == majority_class_name]\n",
    "        df_others = df_copy[df_copy['Component Name'] != majority_class_name]\n",
    "        if len(df_majority) > cap_limit:\n",
    "            df_majority_downsampled = df_majority.sample(n=cap_limit, random_state=42)\n",
    "            df_copy = pd.concat([df_majority_downsampled, df_others], ignore_index=True)\n",
    "\n",
    "    # Filter out single-word observations\n",
    "    df_copy = df_copy[df_copy['Observations'].str.split().str.len() != 1]\n",
    "\n",
    "    # Cascading duplicate limits\n",
    "    df_copy = df_copy[df_copy.groupby(['Component Name', 'Description', 'Observations']).cumcount() < DUPLICATE_LIMIT_TRIO]\n",
    "    df_copy = df_copy[df_copy.groupby(['Description', 'Observations']).cumcount() < DUPLICATE_LIMIT_PAIR]\n",
    "    df_copy = df_copy[df_copy.groupby('Description').cumcount() < DUPLICATE_LIMIT_DESC]\n",
    "    df_copy = df_copy[df_copy.groupby('Observations').cumcount() < DUPLICATE_LIMIT_OBS]\n",
    "\n",
    "    # Final down-sample of \"No Observation\" rows\n",
    "    obs_rows_idx = df_copy.index[df_copy['Observations'] == OBSERVATION_STRING].tolist()\n",
    "    if len(obs_rows_idx) > (df_copy.shape[0] * NA_OBSERVATION_TARGET_PERCENT):\n",
    "        target_keep = int(df_copy.shape[0] * NA_OBSERVATION_TARGET_PERCENT)\n",
    "        indices_to_drop = np.random.choice(obs_rows_idx, size=len(obs_rows_idx) - target_keep, replace=False)\n",
    "        df_copy.drop(indices_to_drop, inplace=True)\n",
    "        \n",
    "    print(f\"Frequency balancing complete. Shape of cleaned data before semantic step: {df_copy.shape}\\n\")\n",
    "    return df_copy\n",
    "\n",
    "def semantic_deduplication(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs advanced de-duplication using semantic clustering.\"\"\"\n",
    "    print(\"--- 5. Advanced Semantic De-duplication ---\")\n",
    "    if df.empty:\n",
    "        print(\"Dataset is empty, skipping semantic de-duplication.\\n\")\n",
    "        return df\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['combined_text'] = df_copy['Component Name'] + ' | ' + df_copy['Description'] + ' | ' + df_copy['Observations']\n",
    "    \n",
    "    print(\"Loading sentence embedding model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(f\"Generating semantic vectors for {len(df_copy)} logs... (This may take a moment)\")\n",
    "    embeddings = model.encode(df_copy['combined_text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    print(\"Reducing vector dimensions with UMAP...\")\n",
    "    reducer = umap.UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    print(\"Clustering logs with HDBSCAN...\")\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True)\n",
    "    clusterer.fit(umap_embeddings)\n",
    "    df_copy['cluster_id'] = clusterer.labels_\n",
    "    \n",
    "    initial_rows = df_copy.shape[0]\n",
    "    df_copy = df_copy[df_copy.groupby('cluster_id').cumcount() < 3]\n",
    "    print(f\"Semantic de-duplication complete. Rows removed: {initial_rows - df_copy.shape[0]}\")\n",
    "    \n",
    "    df_copy.drop(columns=['combined_text', 'cluster_id'], inplace=True)\n",
    "    \n",
    "    print(f\"Shape of the final 'df_cleaned' dataset: {df_copy.shape}\\n\")\n",
    "    return df_copy\n",
    "\n",
    "def create_informative_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates a subset of the data with high-context, dissimilar text pairs.\"\"\"\n",
    "    print(\"--- 6. Creating High-Context 'df_informative' Dataset ---\")\n",
    "    df_info = df.copy()\n",
    "    \n",
    "    df_info = df_info[df_info['Observations'] != OBSERVATION_STRING]\n",
    "    df_info = df_info[df_info['Description'].str.split().str.len() > WORD_COUNT_THRESHOLD]\n",
    "    df_info = df_info[df_info['Observations'].str.split().str.len() > WORD_COUNT_THRESHOLD]\n",
    "    \n",
    "    def calculate_similarity(a, b):\n",
    "        if not isinstance(a, str) or not isinstance(b, str): return 0.0\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    df_info['similarity'] = df_info.apply(lambda row: calculate_similarity(row['Description'], row['Observations']), axis=1)\n",
    "    df_info = df_info[df_info['similarity'] < DISSIMILARITY_THRESHOLD]\n",
    "    df_info.drop(columns=['similarity'], inplace=True)\n",
    "    \n",
    "    print(f\"Created 'df_informative' with {df_info.shape[0]} high-context rows.\\n\")\n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cdbfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Successfully loaded. Initial shape: (100, 13)\n",
      "\n",
      "--- 2. Initial Filtering & Cleaning ---\n",
      "Initial filtering and cleaning complete.\n",
      "\n",
      "--- 3. Removing Redundant Rows ---\n",
      "Redundancy removal complete.\n",
      "\n",
      "--- 4. Frequency Balancing & Final Content Filtering ---\n",
      "Frequency balancing complete. Shape of cleaned data before semantic step: (17, 13)\n",
      "\n",
      "--- 5. Advanced Semantic De-duplication ---\n",
      "Loading sentence embedding model...\n",
      "Generating semantic vectors for 17 logs... (This may take a moment)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf618f907d943a1bd2aa9056558bf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing vector dimensions with UMAP...\n",
      "Clustering logs with HDBSCAN...\n",
      "Semantic de-duplication complete. Rows removed: 10\n",
      "Shape of the final 'df_cleaned' dataset: (7, 13)\n",
      "\n",
      "--- 6. Creating High-Context 'df_informative' Dataset ---\n",
      "Created 'df_informative' with 1 high-context rows.\n",
      "\n",
      "--- 7. Saving All Outputs ---\n",
      "Saved the large cleaned dataset to 'updated_cleaned_data.csv'.\n",
      "Saved the high-context dataset to 'informative_data.csv'.\n",
      "\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# --- Main Execution Pipeline ---\n",
    "# =====================================================================================\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"--- 1. Loading Data ---\")\n",
    "try:\n",
    "    df_raw = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Successfully loaded. Initial shape: {df_raw.shape}\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{INPUT_FILE}'.\\n\")\n",
    "    exit()\n",
    "\n",
    "# 2. Run the full cleaning and balancing pipeline\n",
    "df_cleaned = initial_filter_and_clean(df_raw)\n",
    "df_cleaned = remove_redundancy(df_cleaned)\n",
    "df_cleaned = balance_frequencies(df_cleaned)\n",
    "df_cleaned = semantic_deduplication(df_cleaned)\n",
    "\n",
    "# 3. Create the informative subset from the final cleaned data\n",
    "df_informative = create_informative_dataset(df_cleaned)\n",
    "\n",
    "# 4. Save final outputs\n",
    "print(\"--- 7. Saving All Outputs ---\")\n",
    "df_cleaned.to_csv(CLEANED_OUTPUT_FILE, index=False)\n",
    "print(f\"Saved the large cleaned dataset to '{CLEANED_OUTPUT_FILE}'.\")\n",
    "df_informative.to_csv(INFORMATIVE_OUTPUT_FILE, index=False)\n",
    "print(f\"Saved the high-context dataset to '{INFORMATIVE_OUTPUT_FILE}'.\")\n",
    "\n",
    "print(\"\\nProcess completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
